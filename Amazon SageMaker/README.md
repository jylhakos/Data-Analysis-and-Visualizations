# Amazon SageMaker

To use Amazon SageMaker for Large Language Model (LLM) inference, you'll deploy your LLM to a SageMaker endpoint using a container, like the Large Model Inference (LMI) container. 

LMI container optimizes performance for LLMs, leveraging libraries like vLLM.

Quantizing your model (e.g., using AWQ, GPTQ, or FP8) can reduce memory usage and improve inference speed, but may affect output quality.

**References**

- **What is Amazon SageMaker AI?**: https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html
- **Run Agent**: https://docs.aws.amazon.com/sagemaker/latest/dg/edge-getting-started-step5.html

