# AWS Glue ETL Dockerfile
# ====================
# Multi-stage build for weather ETL microservices with AWS Glue support

FROM amazonlinux:2 AS glue-base

# Install system dependencies for AWS Glue
RUN yum update -y && \
    yum install -y \
        python3 \
        python3-pip \
        java-1.8.0-openjdk \
        wget \
        unzip \
        which \
        tar \
        gzip && \
    yum clean all

# Set JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk
ENV PATH=$PATH:$JAVA_HOME/bin

# Install Spark (compatible with AWS Glue)
ENV SPARK_VERSION=3.4.1
ENV HADOOP_VERSION=3
RUN cd /opt && \
    wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip

# Production stage
FROM glue-base AS production

# Set working directory
WORKDIR /app

# Copy requirements first for better caching
COPY requirements.txt .

# Install Python dependencies
RUN python3 -m pip install --upgrade pip && \
    python3 -m pip install -r requirements.txt

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p /app/logs /app/data /app/checkpoints

# Set environment variables
ENV PYTHONPATH=/app:$PYTHONPATH
ENV AWS_DEFAULT_REGION=us-east-1
ENV PYSPARK_PYTHON=python3
ENV PYSPARK_DRIVER_PYTHON=python3

# Expose ports
EXPOSE 50051 50052 50053 8000 8080

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python3 -c "import requests; requests.get('http://localhost:8000/health')" || exit 1

# Default command
CMD ["python3", "services/enhanced_etl_processing.py"]
